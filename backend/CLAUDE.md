# Backend CLAUDE.md

This file provides comprehensive guidance to Claude Code when working with the FastAPI backend for the SAE Feature Visualization project.

## Project Status: ✅ PRODUCTION-READY RESEARCH PROTOTYPE

The backend is a production-ready FastAPI application with V2 classification engine, supporting dynamic tree building and flexible split rule evaluation optimized for research demonstrations.

## Architecture Overview

### 🏗️ Three-Tier Architecture (✅ FULLY IMPLEMENTED)

```
┌─────────────────────────────────────────────────────────────────┐
│                     FastAPI Application Layer                   │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐   │
│  │   API Router    │ │  Exception      │ │   CORS &        │   │
│  │   (5 Endpoints) │ │  Handling       │ │   Lifespan      │   │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
                                 ↕
┌─────────────────────────────────────────────────────────────────┐
│                     Service Layer                               │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐   │
│  │   DataService   │ │   Async Init    │ │   Filter Cache  │   │
│  │   (Polars)      │ │   & Cleanup     │ │   & Validation  │   │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
                                 ↕
┌─────────────────────────────────────────────────────────────────┐
│                       Data Layer                                │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐   │
│  │ Master Parquet  │ │   Lazy Frame    │ │  String Cache   │   │
│  │ (1,648 features)│ │   Evaluation    │ │   Enabled       │   │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
```

## Technology Stack

### Core Dependencies (Production Versions)
```
FastAPI 0.104.1          # High-performance async web framework
Uvicorn 0.24.0          # ASGI server with hot reload support
Polars 0.19.19          # Lightning-fast columnar data processing
Pydantic 2.5.0          # Data validation and serialization
NumPy 1.25.2            # Numerical computing foundation
```

### Development & Testing
```
pytest 7.4.3            # Testing framework
pytest-asyncio 0.21.1   # Async test support
httpx 0.25.2            # HTTP client for testing
python-multipart 0.0.6  # Form data support
```

## Project Structure

```
backend/
├── app/
│   ├── main.py                    # 🚀 FastAPI application with lifespan management
│   ├── api/
│   │   ├── __init__.py           # 📡 API router aggregation
│   │   ├── filters.py            # ✅ GET /api/filter-options
│   │   ├── histogram.py          # ✅ POST /api/histogram-data
│   │   ├── sankey.py             # ✅ POST /api/sankey-data
│   │   ├── comparison.py         # ✅ POST /api/comparison-data (Phase 2)
│   │   └── feature.py            # ✅ GET /api/feature/{id}
│   ├── models/                   # 📋 Pydantic model definitions
│   │   ├── requests.py           # Request schemas with validation
│   │   ├── responses.py          # Response schemas with type safety
│   │   └── common.py             # Shared models and enums
│   └── services/
│       ├── data_service.py       # 🏭 High-performance Polars data service
│       ├── classification.py      # 🔧 V2 classification engine
│       ├── split_evaluators.py    # ⚙️ Split rule evaluation logic
│       └── data_constants.py      # 📊 Data schema constants
├── docs/                         # 📚 API documentation
├── start.py                      # 🔧 Production startup script with CLI args
├── test_api.py                   # 🧪 Comprehensive API testing suite
├── requirements.txt              # 📦 Dependency specifications
└── README.md                     # 📖 Complete API documentation
```

## Development Commands

### Quick Start (Development)
```bash
cd backend

# Install dependencies
pip install -r requirements.txt

# Start with auto-reload and debug logging (default port 8003)
python start.py --reload --log-level debug

# Start on custom port with specific logging
python start.py --port 8001 --reload --log-level info
```

### Production Deployment
```bash
# Production mode (all interfaces, no reload)
python start.py --host 0.0.0.0 --port 8000

# Multi-worker production (external ASGI server)
uvicorn app.main:app --workers 4 --host 0.0.0.0 --port 8000
```

### Testing & Validation
```bash
# Run comprehensive API tests
python test_api.py

# Test specific port
python test_api.py --port 8003

# Manual health check
curl http://localhost:8003/health
```

## API Endpoints (✅ ALL IMPLEMENTED & TESTED)

### Core Visualization Endpoints

| Method | Endpoint | Purpose | Status |
|--------|----------|---------|--------|
| `GET` | `/api/filter-options` | Dynamic filter population for UI | ✅ Cached |
| `POST` | `/api/histogram-data` | Threshold slider visualization | ✅ Optimized |
| `POST` | `/api/sankey-data` | Phase 1 flow diagrams | ✅ Multi-stage |
| `POST` | `/api/comparison-data` | Phase 2 alluvial comparisons | ✅ Ready |
| `GET` | `/api/feature/{id}` | Debug view detail drilling | ✅ JSON linked |

### System Endpoints

| Method | Endpoint | Purpose | Features |
|--------|----------|---------|----------|
| `GET` | `/health` | Service monitoring | Data service status |
| `GET` | `/` | API information | Version & description |
| `GET` | `/docs` | Interactive API docs | Swagger UI |
| `GET` | `/redoc` | Alternative docs | ReDoc interface |

## Data Service Architecture

### 🏭 DataService Class Features

#### Initialization & Lifecycle
```python
# Async initialization with proper error handling
await data_service.initialize()

# Automatic data file validation
check_data_files()  # Built into start.py

# Graceful cleanup on shutdown
await data_service.cleanup()
```

#### Performance Optimizations
- **Lazy Frame Evaluation**: Queries planned before execution
- **String Cache Enabled**: Categorical data optimized
- **Filter Option Caching**: Pre-computed unique values
- **Async Operations**: Non-blocking I/O throughout
- **Memory Efficient**: Large datasets handled without loading full data

#### Data Processing Pipeline

**Stage 1: Feature Splitting**
```
Features → [feature_splitting: True|False] → Category Groups
```

**Stage 2: Semantic Distance Classification**
```
Category Groups → [semdist_mean >= threshold] → High/Low Distance
```

**Stage 3: Score Agreement Analysis**
```
Distance Groups → [fuzz, simulation, detection scores] → 4 Agreement Levels
├── All 3 High    (all scores ≥ threshold)
├── 2 of 3 High   (exactly 2 scores ≥ threshold)
├── 1 of 3 High   (exactly 1 score ≥ threshold)
└── All 3 Low     (all scores < threshold)
```

## Request/Response Architecture

### Type-Safe Request Models
```python
# Example: Sankey data request with full validation
@app.post("/api/sankey-data")
async def generate_sankey_data(request: SankeyDataRequest):
    # Automatic validation via Pydantic
    # Type hints ensure IDE support
    # Error handling with custom codes
```

### Consistent Error Responses
```json
{
  "error": {
    "code": "INVALID_FILTERS",
    "message": "One or more filter values are invalid",
    "details": {
      "sae_id": ["unknown_sae_id_value"]
    }
  }
}
```

### Error Code Catalog
- `INVALID_FILTERS` - Filter validation failed
- `INVALID_THRESHOLDS` - Threshold values out of range
- `INSUFFICIENT_DATA` - No features match criteria
- `FEATURE_NOT_FOUND` - Requested feature doesn't exist
- `SERVICE_UNAVAILABLE` - Data service not ready
- `INTERNAL_ERROR` - Unexpected server error

## CORS & Frontend Integration

### Multi-Port Frontend Support
```python
allow_origins=[
    "http://localhost:3000",   # React dev server default
    "http://localhost:3003",   # Current frontend port
    "http://localhost:3004",   # Frontend fallback port
    "http://localhost:5173",   # Vite default port
    "http://127.0.0.1:3000",   # IPv4 localhost variants
    "http://127.0.0.1:3003",
    "http://127.0.0.1:3004",
    "http://127.0.0.1:5173"
]
```

## Production Features

### 🚀 Advanced FastAPI Features
- **Lifespan Management**: Proper startup/shutdown hooks
- **Exception Handling**: Global error handlers with structured responses
- **Automatic Documentation**: OpenAPI 3.0 with interactive Swagger UI
- **Request Validation**: Pydantic models with detailed error messages
- **Logging Integration**: Structured logging with configurable levels

### 🔧 Startup Script (start.py)
- **CLI Argument Parsing**: Flexible host/port/logging configuration
- **Data File Validation**: Pre-startup data availability checks
- **Interactive Prompts**: Graceful handling of missing data files
- **Environment Detection**: Development vs production mode handling

### 🧪 Comprehensive Testing (test_api.py)
- **APITester Class**: Systematic endpoint validation
- **Health Monitoring**: Service availability verification
- **Data Pipeline Testing**: End-to-end request/response validation
- **Error Scenario Coverage**: Invalid inputs and edge cases

## Data Requirements & Schema

### Master Parquet File
- **Location**: `/data/master/feature_analysis.parquet`
- **Schema**: 1,648 features with complete metadata
- **Columns**: feature_id, sae_id, explanation_method, llm_explainer, llm_scorer, feature_splitting, semdist_mean, score_fuzz, score_simulation, score_detection, details_path

### Detailed JSON Files
- **Location**: `/data/detailed_json/`
- **Format**: Individual feature files referenced by `details_path`
- **Content**: Complete feature analysis data for debug view

## Performance Characteristics

### Current Deployment Status
- **Dataset Size**: 1,648 features processed
- **Response Times**: Sub-second for all endpoints
- **Memory Usage**: Efficient lazy loading prevents large footprint
- **Scalability**: Designed for 16K+ features
- **Concurrency**: Async/await throughout for high throughput

### Performance Metrics
```
Filter Options:     ~50ms    (cached)
Histogram Data:     ~200ms   (with 20 bins)
Sankey Generation:  ~300ms   (full pipeline)
Feature Details:    ~10ms    (direct lookup)
Health Check:       ~5ms     (service status)
```

## Development Guidelines

### Code Quality Standards
1. **Type Safety**: All functions have complete type hints
2. **Async Patterns**: Proper async/await usage throughout
3. **Error Handling**: Comprehensive exception handling with user-friendly messages
4. **Documentation**: Docstrings for all public methods and classes
5. **Testing**: New endpoints require test coverage in test_api.py

### Adding New Endpoints
1. Create endpoint module in `app/api/endpoints/`
2. Define request/response models in `app/models/`
3. Add router import to `app/api/__init__.py`
4. Add test cases to `test_api.py`
5. Update API documentation

### Database Migration Path
For future scaling beyond Parquet:
- Index on filter columns (sae_id, explanation_method, etc.)
- Index on feature_id for direct lookups
- Consider partitioning by sae_id for large datasets
- Maintain Polars compatibility for existing queries

## Current Server Status

### 🟢 Production Deployments
- **Primary**: Port 8003 (matches frontend expectations)
- **Secondary**: Port 8001 (development/testing)
- **Status**: Multiple servers running simultaneously with active API traffic
- **Health**: All endpoints operational and tested with sub-second response times
- **Performance**: Handling hundreds of concurrent API requests

### 🔍 Monitoring & Observability
- **Health Endpoint**: `/health` shows data service connectivity and master file status
- **Structured Logging**: Configurable levels (debug/info/warning/error) with detailed classification logs
- **Error Tracking**: Full stack traces for debugging with structured error responses
- **Request Logging**: Automatic access log generation with API endpoint performance metrics
- **Feature Classification Logging**: Detailed logs for flexible threshold tree V2 processing and feature distribution changes

## Advanced Implementation Details

### 🧠 V2 Classification Engine (Production Implementation)

The backend implements a **production-ready V2 classification engine** with modular architecture:

#### Core Components:
- **ClassificationEngine** (`classification.py`): Main classification orchestrator
  - `classify_features()`: Complete feature classification using threshold tree
  - `filter_features_for_node()`: Node-specific feature filtering for histograms
  - `build_sankey_data()`: Sankey diagram data generation
- **SplitEvaluator** (`split_evaluators.py`): Split rule evaluation
  - `evaluate_range_split()`: Range-based splits (N thresholds → N+1 branches)
  - `evaluate_pattern_split()`: Pattern-based splits (multi-metric conditions)
  - `evaluate_expression_split()`: Expression-based splits (logical conditions)
- **Dynamic Tree Support**: Runtime stage creation/removal through threshold tree structure

#### Flexible Split Rule Types (New in V2):

**1. Range Rules (Single metric, multiple thresholds)**
```python
RangeSplitRule(
    type="range",
    metric="semdist_mean",
    thresholds=[0.1, 0.3, 0.6]  # Creates 4 branches automatically
)
```
- **Use Case**: Traditional threshold-based splitting
- **Flexibility**: N thresholds create N+1 branches automatically
- **Research Value**: Easy to modify thresholds for different experimental conditions

**2. Pattern Rules (Multi-metric pattern matching)**
```python
PatternSplitRule(
    type="pattern",
    conditions={
        "score_fuzz": PatternCondition(threshold=0.8),
        "score_simulation": PatternCondition(threshold=0.7),
        "score_detection": PatternCondition(threshold=0.8)
    },
    patterns=[
        Pattern(match={"score_fuzz": "high", "score_simulation": "high", "score_detection": "high"},
               child_id="all_high"),
        Pattern(match={"score_fuzz": "high", "score_simulation": "high"},
               child_id="two_high"),
        # ... configurable patterns for any number of metrics
    ]
)
```
- **Use Case**: Complex multi-metric conditions (replaces hardcoded "2 of 3 high" logic)
- **Flexibility**: Support any number of metrics and conditions
- **Research Value**: Create custom agreement patterns for different research scenarios

**3. Expression Rules (Logical expressions)**
```python
ExpressionSplitRule(
    type="expression",
    available_metrics=["score_fuzz", "score_simulation", "score_detection"],
    branches=[
        ExpressionBranch(
            condition="(score_fuzz > 0.8 && score_simulation > 0.7) || score_detection > 0.9",
            child_id="high_confidence"
        )
    ],
    default_child_id="low_confidence"
)
```
- **Use Case**: Advanced research scenarios with complex logical conditions
- **Flexibility**: Unlimited logical complexity
- **Research Value**: Express sophisticated research hypotheses as classification rules

#### Threshold Tree Structure V2:
```python
class ThresholdStructure(BaseModel):
    nodes: List[SankeyThreshold]  # All nodes with embedded split rules
    metrics: List[str]           # Available metrics
    version: int = 2             # V2 system identifier

class SankeyThreshold(BaseModel):
    id: str                      # Node identifier
    stage: int                   # Stage in pipeline (configurable order)
    category: CategoryType       # Visualization category
    parent_path: List[ParentPathInfo]  # Complete path from root
    split_rule: Optional[SplitRule]    # Embedded split configuration
    children_ids: List[str]      # Child node references
```

#### Research-Oriented Implementation:

**Configurable Classification Pipeline**
```
Raw Features → Stage 1 (Configurable) → Stage 2 (Configurable) → ... → Stage N (Configurable) → Final Sankey Nodes

Example Current Configuration:
Raw Features → Feature Splitting (Range) → Semantic Distance (Range) → Score Agreement (Pattern) → Final Nodes

Example Alternative Configuration:
Raw Features → Score Agreement (Pattern) → Feature Splitting (Range) → Semantic Distance (Range) → Final Nodes

Note: Stage order is configurable through threshold tree structure - no code changes required
```

**Conference Demonstration Features:**
- **Real-time Reconfiguration**: Modify classification logic during presentations
- **Research Scenario Testing**: Switch between different research hypotheses instantly
- **Flexible Metrics**: Add/remove scoring methods through configuration
- **Maintainable**: Avoid over-engineering while supporting complex research needs


### 🔧 Data Service Architecture

#### Core Components
1. **DataService**: Research-optimized service class with flexible classification
   - Configurable feature classification algorithms supporting all split rule types (Range, Pattern, Expression)
   - Dynamic threshold tree V2 processing and validation
   - Flexible Sankey diagram data structure building
   - Research-oriented data processing operations optimized for demonstrations

#### Advanced Features
- **Lazy DataFrame Operations**: All operations use Polars LazyFrame for memory efficiency
- **String Cache Optimization**: Categorical data operations optimized with string cache
- **Flexible Split Rule Processing**: Dynamic handling of Range, Pattern, and Expression rules
- **Parent Path Tracking**: Complete path information from root to any node in threshold tree V2
- **Classification State Logging**: Detailed logs of feature distribution changes at each configurable stage
- **Error Context Preservation**: Comprehensive error handling with context information for research scenarios

### 📊 Performance Optimizations

#### Memory Efficiency
- **Lazy Evaluation**: Queries planned and optimized before execution
- **Columnar Processing**: Polars columnar operations for vectorized computations
- **Selective Column Loading**: Only required columns loaded for each operation
- **Temporary Column Cleanup**: Intermediate classification columns dropped after use

#### Query Optimization
- **Filter Pushdown**: Filters applied at the LazyFrame level for early elimination
- **Aggregation Optimization**: Group-by operations optimized for categorical data
- **Index Utilization**: String cache enables efficient categorical operations
- **Batch Processing**: Multiple histogram requests processed in batches

### 🏗️ Modular Service Architecture

#### Service Layer Architecture
```
API Endpoints → DataService → ClassificationEngine → SplitEvaluator → Data Processing
```

- **Endpoint Layer**: Request validation, response formatting, error handling
- **Service Layer**: DataService orchestrates data operations
- **Classification Layer**: ClassificationEngine handles feature classification
- **Evaluation Layer**: SplitEvaluator evaluates split rules
- **Data Layer**: Polars operations, file I/O, caching

#### Key Design Patterns
- **Dependency Injection**: DataService injected into endpoints via FastAPI dependencies
- **Factory Pattern**: DataService methods construct complex data structures
- **Strategy Pattern**: Different threshold application strategies
- **Observer Pattern**: Logging and monitoring throughout classification pipeline

## Integration Points

### Frontend Compatibility
- **API Base URL**: Configurable via environment variables
- **CORS Headers**: Pre-configured for all common development ports
- **Error Responses**: Structured format matching frontend expectations
- **Data Format**: JSON responses optimized for React/D3.js consumption

### Data Pipeline Integration
- **Parquet Files**: Direct integration with preprocessing pipeline
- **JSON Details**: Linked detailed data for feature drilling
- **Metadata Files**: Schema validation and documentation
- **File Validation**: Startup checks for data availability

## Future Enhancement Roadmap

### ✅ Completed Features
- ✅ **V2 Classification Engine**: Modular classification with split evaluators
- ✅ **Dynamic Tree Support**: Runtime stage creation/removal
- ✅ **Comparison Endpoint**: Alluvial flow data generation
- ✅ **Node Filtering**: Histogram data filtered by node path

### Performance Optimizations
- 📝 Request rate limiting implementation
- 📝 Redis caching for frequent queries
- 📝 Database backend option for larger datasets
- 📝 Batch processing for bulk operations

### Security & Monitoring
- 📝 API key authentication system
- 📝 Request/response logging enhancement
- 📝 Metrics collection and alerting
- 📝 Health check endpoint expansion

## Critical Notes for Development

1. **Data Dependency**: Backend requires master parquet file to function
2. **Port Configuration**: Default 8003 matches frontend environment
3. **Polars Version**: String cache compatibility requires exact version
4. **Async Patterns**: All data operations are async - maintain this pattern
5. **Error Handling**: Use custom error codes for frontend error handling
6. **CORS Setup**: Frontend ports pre-configured - update for new ports
7. **Testing**: Always run test_api.py after changes to verify functionality

The backend represents a research prototype implementation with flexible, configurable architecture, reliable error handling, and demonstration optimizations suitable for academic conference presentations and SAE research scenarios.

**Key Implementation Features:**
- **V2 Classification Engine**: Modular classification with `ClassificationEngine` and `SplitEvaluator`
- **Dynamic Tree Support**: Runtime stage creation/removal through threshold tree structure
- **Three Split Rule Types**: Range, pattern, and expression-based splitting
- **Production-Ready**: Comprehensive error handling and logging
- **Research Flexibility**: Support diverse research scenarios through configuration
- **Conference Optimized**: Reliable performance for live academic demonstrations
- **Maintainable Architecture**: Clear separation of concerns with modular design